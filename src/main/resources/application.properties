spring.application.name=demo
server.port=8081

#LLM model configuration
# for Ollama
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.options.model=llama3.1:latest
spring.ai.ollama.chat.options.temperature=0.3

#for openAI
spring.ai.openai.api-key="sampleKey"
spring.ai.openai.chat.options.model=gpt-4.1
spring.ai.openai.chat.options.temperature=0.3

#logging
logging.level.org.springframework.ai.chat.client.advisor=DEBUG


